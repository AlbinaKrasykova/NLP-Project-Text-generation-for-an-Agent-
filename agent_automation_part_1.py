# -*- coding: utf-8 -*-
"""Agent automation part -1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rpu5L0bC__V7gTVCDk7fm-Qr3Dgm7njW

**First Part of Agent Automation Project**
consists of:
 1. Data preprocessing 
 2. Exploratory Analysis 
 3. Feature Enginerring 
 4. Text clustering 
 5. Working on the strategy Development for the Challenge, and dividing roles to make the workflow more efficient
 6. Saving clusters to a file

**Importing libraries **
"""

# down forget to upload abcd file to the source file in here, chose 'upload the session storage' *
# and download abcd file to your computer, it is right in the folder too *

import json
import pandas as pd
from pandas import json_normalize
import tensorflow as tf 

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from scipy import stats

import os
import pandas as pd
!pip install -U sentence-transformers
from sentence_transformers import SentenceTransformer
embedder = SentenceTransformer('distilbert-base-nli-mean-tokens')

import re
!pip install preprocessor
import preprocessor as p

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from wordcloud import WordCloud

!pip install tweet-preprocessor
import preprocessor as p

"""Loading and opening data file with json.loads and creating a dataframe """

with open('abcd.json','r') as f:
 data = json.loads(f.read())

data_train = data['train']
data_dev = data['dev']
data_test = data['test']

df_3= pd.json_normalize(data_train, record_path=['delexed'], meta = [['convo_id'],['scenario','personal'],['scenario','order'],['scenario','product'],['scenario','flow'],['scenario','subflow']])

df_3.columns
df_3.shape[0]

df_3.isna().sum()

#df_20 = df_train.sample(n = 35286, random_state = 2)
#df_20 = df_20.sample(n = 5000, random_state = 5)
#df_20.sample(frac=0.2, replace=True, random_state=1)
#correlation matrix

"""Dropping the less important features """

colnames_to_drop = ['turn_count','targets', 'candidates', 'scenario.order', 'scenario.product', 'scenario.personal','convo_id'] 
df3 = pd.DataFrame(df_3.drop(colnames_to_drop, axis=1))

"""Updating the frame """

df3

"""**Feature Engineering Part**. Extracting and separating specifically text for agent and custumer. (creating 2 new datframes with extratced texta nd then mering it using function merge and parameter how='inner' )"""

#df_20
#colnames_to_drop = ['turn_count','targets', 'candidates', 'scenario.order', 'scenario.product', 'scenario.personal','scenario.subflow'] 
#df3 = pd.DataFrame(df_20.drop(colnames_to_drop, axis=1))
df3 = df3.rename(columns={'scenario.flow':"category"})
df3 = df3.rename(columns={'scenario.subflow':"sub-category"})

df_agent = df3[df3['speaker'] == 'agent']
df_agent
df_customer = df3[df3['speaker'] == 'customer']
df_customer

sorted_df =pd.DataFrame(columns = ['df_agent'])
#cluster_df = pd.DataFrame(corpus, columns = ['cleaned_text_of_agent_for_clusters'])

df_agent.reset_index(drop=True, inplace=True) #reset the index
df_customer.reset_index(drop=True, inplace=True)
df_merged = pd.merge( df_agent, df_customer, left_index=True, right_index=True, how='inner', suffixes=('_agent', '_customer'))
df_merged.head(4)

df_merged.isna().sum()

# renaming columns flow to the 'category'


df_merged

colnames_to_drop = ['category_customer','sub-category_customer','speaker_agent'] 
df_merged = pd.DataFrame(df_merged.drop(colnames_to_drop, axis=1))

df_merged.isna().sum()

#new df
df = pd.DataFrame(df_merged, columns = ['cleaned_text_of_agent_for_clusters'])

df_merged.dtypes

# Shuffel the dataset to make sure we get an equal distribution of the data before splitting into train and test sets
#dataset = df.sample(frac=1)
#dataset.head(4)
colnames_to_drop = ['turn_count','targets', 'candidates', 'scenario.order', 'scenario.product', 'scenario.personal','convo_id'] 
df3 = pd.DataFrame(df_3.drop(colnames_to_drop, axis=1))

sorted = df_merged.groupby('category_agent')
# Let's print the first entries
# in all the groups formed.
sorted.first()

"""**Visualizing most popular categories for the angent workflow with seaborn**"""

import seaborn as sns
ax = sns.countplot(x="category_agent", data=df_merged)

ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha="right")
plt.tight_layout()
plt.show()

"""**Clustering Part**. Data preprocesing

Cleaning the data
"""

# custum function to clean the dataset (combining tweet_preprocessor and reguar expression)
def clean_text(df4):
    #set up punctuations we want to be replaced
    REPLACE_NO_SPACE = re.compile("(\.)|(\;)|(\:)|(\!)|(\')|(\?)|(\,)|(\")|(\|)|(\()|(\))|(\[)|(\])|(\%)|(\$)|(\>)|(\<)|(\{)|(\})")
    REPLACE_WITH_SPACE = re.compile("(<br\s/><br\s/?)|(-)|(/)|(:).")
    tempArr = []
    for line in df4:
        # send to tweet_processor
        tmpL = p.clean(line)
        # remove puctuation
        tmpL = REPLACE_NO_SPACE.sub("", tmpL.lower()) # convert all tweets to lower cases
        tmpL = REPLACE_WITH_SPACE.sub(" ", tmpL)
        tempArr.append(tmpL)
    return tempArr

# Cleaning text for Agent 

import preprocessor.api as p
from preprocessor.api import clean, tokenize, parse

# cleaning the text
df_merged['clean _text_agent'] = clean_text(df_merged['text_agent'])


df_merged.head()
#fix teh dataframe, so the column can match

corpus = list(df_merged['clean _text_agent'])

corpus = corpus[0:10000]

#500 corpus id clean text of an agent 

# not enough of examples

#cluster all the date 1k
#jupyter

corpus_embeddings = embedder.encode(corpus)

num_clusters = 435
clustering_model = KMeans(n_clusters=num_clusters)
clustering_model.fit(corpus_embeddings)
cluster_assignment = clustering_model.labels_

cluster_df = pd.DataFrame(corpus, columns = ['cleaned_text_of_agent_for_clusters'])
cluster_df['cluster_for_agent'] = cluster_assignment
cluster_df.head()

cluster_df.value_counts

#take 500 values of the whole df
#df_500 = df_merged.head(500)

#take all values 
df = df_merged

#merge cluster df and our df
df = pd.merge( df, cluster_df, left_index=True, right_index=True, how='inner', suffixes=('_agent', '_customer'))
df.head(5)



df['clean_text_agent_corpus'] = clean_text(df['text_agent'])
df.head()

csv_data = df.to_csv()

#convert and chck all the variables 
df.dtypes


#df5['cluster_for_agent']=df5['cluster_for_agent'].astype('int')

df.isna().sum()

"""**Saving the resulted clusters to a file **"""

with open('csv_data.txt', 'w') as csv_file:
    df.to_csv(path_or_buf=csv_file)

"""**Strategy Development for the Challenge, and dividing roles to make the workflow more efficient**

**Strategy Development**
"""

#1) Data prep - Jacquie (try both methods, and see what works best in your opinion) 
#2) lg - Albina  (build lg  model and properly classify custumer sentences based on cluster) 
#-----------
#3 confusion matrix - Kelly (model evaluation)  
#4 picking 3 sentences from cluster - Samihah/Alaysia + Kelly  
#4 (imagine you have a model that let's say classified custumer sentence by 54, pick top 3 sentences from the cluster 54, 
#4  as an option use cosine similarity)
# *Final presentation Dec 16 early time or evening time -  submit the time via email ( Done+ )
# -------
# gtp-2 (easy, we can try that later; feed the data to the model and generate the text base on it, find a metrics )